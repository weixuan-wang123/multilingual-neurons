{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdda9179-388d-44e8-b7dd-6d9e311d19d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bloomz-7b1mt\"\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM,GenerationConfig,AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
    "MODEL = 'bloomz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cf9131-d750-4868-a159-0478c62e9a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baukit import Trace, TraceDict\n",
    "\n",
    "def get_out_bloomz(model, prompt, device,index): \n",
    "\n",
    "    model.eval()\n",
    "    MLP_act = [f\"transformer.h.{i}.mlp.gelu_impl\" for i in range(30)]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with TraceDict(model, MLP_act) as ret:\n",
    "            output = model(prompt, output_hidden_states = True,output_attentions=True)\n",
    "        MLP_act_value = [ret[act_value].output for act_value in MLP_act]\n",
    "        return MLP_act_value\n",
    "    \n",
    "    \n",
    "def act_bloom(input_ids):\n",
    "    mlp_act = get_out_bloomz(model,input_ids,model.device,-1)\n",
    "    mlp_act = np.array(mlp_act)\n",
    "    return mlp_act\n",
    "\n",
    "if 'bloom' in MODEL:\n",
    "    LAYERS = model.config.n_layer\n",
    "    Neuron_num = 16384\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3295e117-1521-409b-88db-4e7195d5d0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = ['en','de','es','fr','ru','th','tr','vi','zh','pt']\n",
    "colors = ['cornflowerblue','forestgreen','orange','red','mediumturquoise','midnightblue','brown','moccasin','darkviolet','gold','deeppink','gray','teal','slateblue']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092f6150-531a-40d1-a89c-2a3595f4aeb8",
   "metadata": {},
   "source": [
    "## XNLI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ddcbf9-4460-474e-b789-820bfd30c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_XNLI():\n",
    "    with open(\"./mnli/xnli.test.tsv\") as f:\n",
    "        lines = f.readlines()\n",
    "    copora = [[],[],[],[],[]]\n",
    "    for line in lines[1:]:\n",
    "        line = line.split('\\t')\n",
    "        lang,label,sent1,sent2 = line[0],line[1],line[6],line[7]\n",
    "        if lang in langs:\n",
    "            ind = langs.index(lang)\n",
    "            res = (label,sent1,sent2)\n",
    "            copora[ind].append(res)\n",
    "        \n",
    "    length = len(copora[0])\n",
    "    question_all, answer_all = [],[]\n",
    "    for i in range(length):\n",
    "        question_all.append([])\n",
    "        answer_all.append([])\n",
    "    for ind in range(length):\n",
    "        for l in range(len(copora)):\n",
    "            tup = copora[l]\n",
    "            label,premise,hypothesis = tup[ind][0],tup[ind][1],tup[ind][2]\n",
    "            prompt = f'Take the following as truth: {premise}\\nThen the following statement: \"{hypothesis}\" is \"true\", \"false\", or \"inconclusive\"?'\n",
    "            question_all[ind].append(prompt)\n",
    "            answer_all[ind].append(label)\n",
    "    question_all, answer_all = question_all[:1000], answer_all[:1000]    \n",
    "    return question_all,answer_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f991b8-063f-402f-8626-07e8369d672f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a7f19b0-9702-4e0b-b1b8-3fbc1bf13529",
   "metadata": {},
   "source": [
    "## behaviors of four types of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a556f3ca-a8cd-4cb2-82ba-88ca48534eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import MultipleLocator\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "question_all,answer_all = load_data_XNLI()\n",
    "\n",
    "test_counts = len(question_all)\n",
    "langs_counts = len(question_all[0])\n",
    "\n",
    "all_activated_counts,all_shared_counts,non_counts,specific_counts,all_partial_shared_counts = [],[],[],[] ,[]\n",
    "for ind in range(test_counts):\n",
    "    sents = question_all[ind]\n",
    "    mlp_acts = []\n",
    "    for lang in range(len(sents)):\n",
    "        question = sents[lang]\n",
    "        encodings = tokenizer(question, return_tensors='pt').to('cuda')\n",
    "        input_ids = encodings['input_ids'].to('cuda')\n",
    "\n",
    "        if 'bloom' in MODEL:\n",
    "            mlp_act = act_bloom(input_ids)\n",
    "\n",
    "        mlp_acts.append(mlp_act)\n",
    "\n",
    "    mlp_all_act = []\n",
    "    for i in range(langs_counts):\n",
    "        mlp_act = (mlp_acts[i]>0).astype(int)\n",
    "        mlp_all_act.append(mlp_act)\n",
    "    mlp_all = np.sum(mlp_all_act,axis=0)    \n",
    "\n",
    "    all_activated,all_activated_count = [],[]\n",
    "    for i in range(langs_counts):   \n",
    "        activated,activated_count = [],[]\n",
    "        for ly in range(LAYERS):\n",
    "            act = torch.nonzero(torch.tensor(mlp_all_act[i][ly]==1)).squeeze(1).detach().cpu().numpy()\n",
    "            activated.append(act) \n",
    "            activated_count.append(len(act))\n",
    "        all_activated.append(activated)\n",
    "        all_activated_count.append(activated_count)\n",
    "\n",
    "\n",
    "    all_shared,all_shared_count = [],[]\n",
    "    for ly in range(LAYERS):\n",
    "        mlp_all_act_inter = torch.nonzero(torch.tensor(mlp_all[ly]==langs_counts)).squeeze(1).detach().cpu().numpy()\n",
    "        all_shared.append(mlp_all_act_inter)\n",
    "        all_shared_count.append(len(mlp_all_act_inter))\n",
    "\n",
    "    non,non_count = [],[]\n",
    "    mlp_all = np.sum(mlp_all_act,axis=0)\n",
    "    for ly in range(LAYERS):\n",
    "        mlp_all_act_inter = torch.nonzero(torch.tensor(mlp_all[ly]==0)).squeeze(1).detach().cpu().numpy()\n",
    "        non.append(mlp_all_act_inter)  \n",
    "        non_count.append(len(mlp_all_act_inter))\n",
    "\n",
    "\n",
    "    specific,specific_count = [],[]\n",
    "    for i in range(langs_counts):       \n",
    "        indices_arr1 = np.where(mlp_all_act[i] == 1)\n",
    "        indices_arr2 = np.where(mlp_all == 1)\n",
    "        set_indices_arr1 = set(zip(indices_arr1[0], indices_arr1[1]))\n",
    "        set_indices_arr2 = set(zip(indices_arr2[0], indices_arr2[1]))\n",
    "        intersection = set_indices_arr1.intersection(set_indices_arr2)\n",
    "        if len(intersection) > 0:\n",
    "            rows_intersection, cols_intersection = zip(*intersection)\n",
    "        else:\n",
    "            rows_intersection, cols_intersection = [],[]\n",
    "        row = [[] for _ in range(LAYERS)]\n",
    "        for k in range(len(rows_intersection)):\n",
    "            r,c = rows_intersection[k],cols_intersection[k]\n",
    "            row[r].append(c)\n",
    "        specific.append(row)\n",
    "\n",
    "\n",
    "    all_specific,specific_count = [],[]\n",
    "    for ly in range(LAYERS):\n",
    "        temp = []\n",
    "        for i in range(langs_counts):\n",
    "            temp += specific[i][ly]\n",
    "        all_specific.append(temp)\n",
    "        specific_count.append(len(temp))\n",
    "\n",
    "\n",
    "\n",
    "    # some shared neuron\n",
    "    partial_shared = []\n",
    "    for lg in range(langs_counts):\n",
    "        lg_some = []\n",
    "        for ly in range(LAYERS):\n",
    "            other = all_shared[ly].tolist() + specific[lg][ly]\n",
    "            some = list((set(all_activated[lg][ly])-set(other)))\n",
    "            lg_some.append(some)\n",
    "        partial_shared.append(lg_some)\n",
    "\n",
    "    all_partial_shared,all_partial_shared_count = [],[]\n",
    "    for ly in range(LAYERS):\n",
    "        temp = []\n",
    "        for i in range(langs_counts):\n",
    "            temp += partial_shared[i][ly]\n",
    "        all_partial_shared.append(list(set(temp)))\n",
    "        all_partial_shared_count.append(len(list(set(temp))))\n",
    "\n",
    "    all_activated_counts.append(all_activated_count)\n",
    "    all_shared_counts.append(all_shared_count)\n",
    "    non_counts.append(non_count)\n",
    "    specific_counts.append(specific_count)\n",
    "    all_partial_shared_counts.append(all_partial_shared_count)\n",
    "\n",
    "\n",
    "print(len(all_activated_counts))\n",
    "\n",
    "all_activated_counts,all_shared_counts,non_counts,specific_counts,all_partial_shared_counts = mean(all_activated_counts, axis=0),mean(all_shared_counts, axis=0),mean(non_counts, axis=0),mean(specific_counts, axis=0),mean(all_partial_shared_counts, axis=0)\n",
    "print(len(all_activated_counts),len(all_activated_counts))    \n",
    "\n",
    "\n",
    "legend_font = {\n",
    "'style': 'normal',\n",
    "'size': 6.5,\n",
    "'weight': \"bold\", \n",
    "}\n",
    "\n",
    "# Create the figure\n",
    "fig, ax = plt.subplots(1,2,figsize=(4,2))\n",
    "\n",
    "\n",
    "xLabel=[i+1 for i in range(LAYERS//2)]\n",
    "x = [(i+1)*2 for i in range(LAYERS//2)]\n",
    "\n",
    "ax[0].plot(x,np.array(non_counts[::2])/(Neuron_num/100),colors[0],marker='s',markersize='2.5',label='non-activated')   \n",
    "\n",
    "\n",
    "ax[0].plot(x,np.array(specific_counts[::2])/(Neuron_num/100),colors[3],marker='o', markersize='2.5', label='specific')   \n",
    "ax[0].plot(x,np.array(all_shared_counts[::2])/(Neuron_num/100),colors[1],marker='d',markersize='2.5', label='all-shared')\n",
    "ax[0].plot(x,np.array(all_partial_shared_counts[::2])/(Neuron_num/100),colors[2],marker='<',markersize='2.5', label='partial-shared')\n",
    "\n",
    "ax[0].set_title('by neuron type',fontsize=7,fontweight='bold')\n",
    "ax[0].tick_params(labelsize=8)  \n",
    "ax[0].xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "ax[0].legend(bbox_to_anchor=(0, -0.15), loc=2, ncol=1,prop=legend_font)\n",
    "\n",
    "\n",
    "for k in range(len(all_activated_counts)):\n",
    "    ax[1].plot(x,np.array(all_activated_counts[k][::2])/(Neuron_num/100),colors[k+4],marker='o', linestyle='-',markersize='2', label=langs[k])    \n",
    "\n",
    "ax[1].set_title('by language',fontsize=7,fontweight='bold')\n",
    "ax[1].tick_params(labelsize=8)\n",
    "ax[1].xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "ax[1].legend(bbox_to_anchor=(-0.1, -0.15), loc=2, ncol=3,prop=legend_font)\n",
    "\n",
    "\n",
    "fig.text(0.53, -0.01, 'Layers', ha='center',fontsize=8,fontweight='bold')\n",
    "fig.text(0.03, 0.5, 'Percentage', va='center', rotation='vertical',fontsize=8,fontweight='bold')\n",
    "fig.subplots_adjust(wspace=0.25,hspace=0.35) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bbd723-96c3-4427-b7c0-e448157b22c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9db481a6-dbe6-4085-9903-5b35cfe841dc",
   "metadata": {},
   "source": [
    "# overlap across testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34c1206-1514-4485-8931-ffe3adddd413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import MultipleLocator\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "question_all,answer_all = load_data_XNLI()\n",
    "\n",
    "test_counts = len(question_all)\n",
    "langs_counts = len(question_all[0])\n",
    "\n",
    "all_activated_counts,all_shared_counts,non_counts,specific_counts,all_partial_shared_counts = [],[],[],[] ,[]\n",
    "all_shared_testset,non_testset,specific_testset,partial_shared_testset = [],[],[],[]\n",
    "for ind in range(test_counts):\n",
    "    sents = question_all[ind]\n",
    "    mlp_acts = []\n",
    "    for lang in range(len(sents)):\n",
    "        question = sents[lang]\n",
    "        encodings = tokenizer(question, return_tensors='pt')\n",
    "        input_ids = encodings['input_ids'].to('cuda')\n",
    "\n",
    "        if 'bloom' in model_name:\n",
    "            mlp_act = act_bloom(input_ids)\n",
    "        mlp_acts.append(mlp_act)\n",
    "\n",
    "    mlp_all_act = []\n",
    "    for i in range(langs_counts):\n",
    "        mlp_act = (mlp_acts[i]>0).astype(int)\n",
    "        mlp_all_act.append(mlp_act)\n",
    "    mlp_all = np.sum(mlp_all_act,axis=0)    \n",
    "\n",
    "    # monolingual activated\n",
    "    all_activated,all_activated_count = [],[]\n",
    "    for i in range(langs_counts):   \n",
    "        activated,activated_count = [],[]\n",
    "        for ly in range(LAYERS):\n",
    "            act = torch.nonzero(torch.tensor(mlp_all_act[i][ly]==1)).squeeze(1).detach().cpu().numpy()\n",
    "            activated.append(act) \n",
    "            activated_count.append(len(act))\n",
    "        all_activated.append(activated)\n",
    "        all_activated_count.append(activated_count)\n",
    "\n",
    "    all_shared,all_shared_count,all_shared_index = [],[],[]\n",
    "    for ly in range(LAYERS):\n",
    "        mlp_all_act_inter = torch.nonzero(torch.tensor(mlp_all[ly]==langs_counts)).squeeze(1).detach().cpu().numpy()\n",
    "        all_shared.append(mlp_all_act_inter)\n",
    "        all_shared_count.append(len(mlp_all_act_inter))\n",
    "\n",
    "    shared_index = (mlp_all==langs_counts).astype(int)\n",
    "    non_index = (mlp_all==0).astype(int)\n",
    "    all_shared_testset.append(shared_index)\n",
    "    non_testset.append(non_index)\n",
    "\n",
    "    non,non_count = [],[]\n",
    "    mlp_all = np.sum(mlp_all_act,axis=0)\n",
    "    for ly in range(LAYERS):\n",
    "        mlp_all_act_inter = torch.nonzero(torch.tensor(mlp_all[ly]==0)).squeeze(1).detach().cpu().numpy()\n",
    "        non.append(mlp_all_act_inter)  \n",
    "        non_count.append(len(mlp_all_act_inter))\n",
    "\n",
    "    specific,specific_count = [],[]\n",
    "    for i in range(langs_counts):       \n",
    "        indices_arr1 = np.where(mlp_all_act[i] == 1)\n",
    "        indices_arr2 = np.where(mlp_all == 1)\n",
    "        set_indices_arr1 = set(zip(indices_arr1[0], indices_arr1[1]))\n",
    "        set_indices_arr2 = set(zip(indices_arr2[0], indices_arr2[1]))\n",
    "        intersection = set_indices_arr1.intersection(set_indices_arr2)\n",
    "        if len(intersection) > 0:\n",
    "            rows_intersection, cols_intersection = zip(*intersection)\n",
    "        else:\n",
    "            rows_intersection, cols_intersection = [],[]\n",
    "        row = [[] for _ in range(LAYERS)]\n",
    "        for k in range(len(rows_intersection)):\n",
    "            r,c = rows_intersection[k],cols_intersection[k]\n",
    "            row[r].append(c)\n",
    "        specific.append(row)\n",
    "\n",
    "\n",
    "    all_specific,specific_count = [],[]\n",
    "    specific_index = [[0 for _ in range(Neuron_num)] for _ in range(LAYERS)]\n",
    "    for ly in range(LAYERS):\n",
    "        temp = []\n",
    "        for i in range(langs_counts):\n",
    "            temp += specific[i][ly]\n",
    "            for index in specific[i][ly]:\n",
    "                specific_index[ly][index] = 1\n",
    "        all_specific.append(temp)\n",
    "        specific_count.append(len(temp))\n",
    "    specific_testset.append(specific_index)\n",
    "\n",
    "\n",
    "    partial_shared = []\n",
    "    some_index = [[0 for _ in range(Neuron_num)] for _ in range(LAYERS)]\n",
    "    for lg in range(langs_counts):\n",
    "        lg_some = []\n",
    "        for ly in range(LAYERS):\n",
    "            other = all_shared[ly].tolist() + specific[lg][ly]\n",
    "            some = list((set(all_activated[lg][ly])-set(other)))\n",
    "            lg_some.append(some)\n",
    "        partial_shared.append(lg_some)\n",
    "\n",
    "    all_partial_shared,all_partial_shared_count = [],[]\n",
    "    for ly in range(LAYERS):\n",
    "        temp = []\n",
    "        for i in range(langs_counts):\n",
    "            temp += partial_shared[i][ly]\n",
    "            for index in partial_shared[i][ly]:\n",
    "                some_index[ly][index] = 1\n",
    "        all_partial_shared.append(list(set(temp)))\n",
    "        all_partial_shared_count.append(len(list(set(temp)))) \n",
    "    partial_shared_testset.append(some_index)\n",
    "\n",
    "\n",
    "    all_activated_counts.append(all_activated_count)\n",
    "    all_shared_counts.append(all_shared_count)\n",
    "    non_counts.append(non_count)\n",
    "    specific_counts.append(specific_count)\n",
    "    all_partial_shared_counts.append(all_partial_shared_count)\n",
    "\n",
    "\n",
    "avg_all_activated_counts,avg_all_shared_counts,avg_non_counts,avg_specific_counts,avg_all_partial_shared_counts = mean(all_activated_counts, axis=0),mean(all_shared_counts, axis=0),mean(non_counts, axis=0),mean(specific_counts, axis=0),mean(all_partial_shared_counts, axis=0)\n",
    "sum_all_shared_counts,sum_non_counts,sum_specific_counts,sum_all_partial_shared_counts = np.sum(all_shared_testset, axis=0),np.sum(non_testset, axis=0),np.sum(specific_testset, axis=0),np.sum(partial_shared_testset, axis=0)\n",
    "\n",
    "num = test_counts\n",
    "counts_testset_all_shared,counts_testset_non,counts_testset_specific,counts_testset_partial_shared = [],[],[],[]\n",
    "for j in range(LAYERS):\n",
    "    counts_testset_all_shared.append(sum_all_shared_counts[j].tolist().count(num)) \n",
    "    counts_testset_non.append(sum_non_counts[j].tolist().count(num)) \n",
    "    counts_testset_specific.append(sum_specific_counts[j].tolist().count(num)) \n",
    "    counts_testset_partial_shared.append(sum_all_partial_shared_counts[j].tolist().count(num)) \n",
    "\n",
    "\n",
    "\n",
    "legend_font = {\n",
    "'style': 'normal',\n",
    "'size': 6.5,  # 字号\n",
    "'weight': \"bold\",  # 是否加粗，不加粗\n",
    "}\n",
    "legend_elements = [\n",
    "                       Line2D([0], [0], marker='o', color='w', label='overlap',markerfacecolor='#37A2FF'),\n",
    "                       Line2D([0], [0], marker='o', color='w', label='average',markerfacecolor='#A17DB4'),\n",
    "                      ]\n",
    "\n",
    "\n",
    "# Create the figure\n",
    "fig, ax = plt.subplots(2,2,figsize=(4,4))\n",
    "\n",
    "\n",
    "xLabel=[i+1 for i in range(LAYERS//2)]\n",
    "x = [(i+1)*2 for i in range(LAYERS//2)]#点的横坐标\n",
    "\n",
    "\n",
    "ax[0,0].stackplot(x,np.array(counts_testset_all_shared[::2])/(Neuron_num/100),np.array(avg_all_shared_counts[::2]-counts_testset_all_shared[::2])/(Neuron_num/100),colors=['#37A2FF','#A17DB4'], alpha=0.5)\n",
    "\n",
    "ax[0,0].set_title('All-shared',fontsize=7,fontweight='bold')\n",
    "ax[0,0].tick_params(labelsize=8)  \n",
    "ax[0,0].xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "\n",
    "ax[0,1].stackplot(x,np.array(counts_testset_partial_shared[::2])/(Neuron_num/100),np.array(avg_all_partial_shared_counts[::2]-counts_testset_partial_shared[::2])/(Neuron_num/100),colors=['#37A2FF','#A17DB4'], alpha=0.5)\n",
    "\n",
    "ax[0,1].set_title('Partial-shared',fontsize=7,fontweight='bold')\n",
    "ax[0,1].tick_params(labelsize=8)  \n",
    "ax[0,1].xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "\n",
    "ax[1,0].stackplot(x,np.array(counts_testset_specific[::2])/(Neuron_num/100),np.array(avg_specific_counts[::2]-counts_testset_specific[::2])/(Neuron_num/100),colors=['#37A2FF','#A17DB4'], alpha=0.5)\n",
    "\n",
    "ax[1,0].set_title('Specific',fontsize=7,fontweight='bold')\n",
    "ax[1,0].tick_params(labelsize=8)  \n",
    "ax[1,0].xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "\n",
    "ax[1,1].stackplot(x,np.array(counts_testset_non[::2])/(Neuron_num/100),np.array(avg_non_counts[::2]-counts_testset_non[::2])/(Neuron_num/100),colors=['#37A2FF','#A17DB4'], alpha=0.5)\n",
    "\n",
    "ax[1,1].set_title('Non-activated',fontsize=7,fontweight='bold')\n",
    "ax[1,1].tick_params(labelsize=8)  \n",
    "ax[1,1].xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "\n",
    "\n",
    "\n",
    "fig.text(0.53, 0, 'Layers', ha='center',fontsize=8,fontweight='bold')\n",
    "fig.text(0.03, 0.5, 'Percentage', va='center', rotation='vertical',fontsize=8,fontweight='bold')\n",
    "fig.subplots_adjust(wspace=0.25,hspace=0.35) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cc307f-9195-451e-abcc-a5bccc01655b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76cc7c17-df45-4eec-baca-0e21313e9ed9",
   "metadata": {},
   "source": [
    "## across every token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31d2b72-ff41-4f82-86a1-b181f6ef1320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_out_all_bloomz(model, prompt, device): \n",
    "\n",
    "    model.eval()\n",
    "    MLP_act = [f\"transformer.h.{i}.mlp.gelu_impl\" for i in range(30)]   \n",
    "    with torch.no_grad():\n",
    "        with TraceDict(model, MLP_act) as ret:\n",
    "            output = model(prompt, output_hidden_states = True,output_attentions=True)\n",
    "        MLP_act_value = [ret[act_value].output[0].detach().cpu().numpy()  for act_value in MLP_act]\n",
    "\n",
    "    return MLP_act_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec93eb33-6f41-4fbc-8fc4-fcb09750e445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import MultipleLocator\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "\n",
    "question_all,answer_all = load_data_XNLI()\n",
    "\n",
    "test_counts = len(question_all)\n",
    "langs_counts = len(question_all[0])\n",
    "\n",
    "\n",
    "counts_each, counts_inter, counts_en_sub, counts_sub_en = [],[],[],[]    \n",
    "mlp_act_all_sum = []\n",
    "non_counts = []\n",
    "for ind in range(test_counts):\n",
    "    sents = question_all[ind]\n",
    "    mlp_acts = []\n",
    "    for sent in sents:\n",
    "        encodings = tokenizer(sent, return_tensors='pt')\n",
    "        input_ids = encodings['input_ids'].to('cuda')\n",
    "        attention_mask = encodings['attention_mask'].to('cuda')\n",
    "        mlp_act = get_out_all_bloomz(model,input_ids,model.device)\n",
    "        mlp_act = np.array(mlp_act)\n",
    "        mlp_act = (mlp_act>0).astype(int)\n",
    "\n",
    "        mlp_act_sum = np.sum(mlp_act,axis=1)\n",
    "        mlp_act_all_sum.append(mlp_act_sum)\n",
    "\n",
    "    mlp_all = np.sum(mlp_act_all_sum,axis=0)  \n",
    "    non,non_count = [],[]    \n",
    "    for ly in range(LAYERS):\n",
    "        mlp_non_inter = torch.nonzero(torch.tensor(mlp_all[ly]==0)).squeeze(1).detach().cpu().numpy()\n",
    "        non.append(mlp_non_inter)  \n",
    "        non_count.append(len(mlp_non_inter))\n",
    "    non_counts.append(non_count)\n",
    "\n",
    "non_counts = mean(non_counts, axis=0)   \n",
    "print(np.array(non_counts).shape)\n",
    "\n",
    "\n",
    "legend_font = {\n",
    "    'style': 'normal',\n",
    "    'size': 6.5, \n",
    "    'weight': \"bold\",  \n",
    "}\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker='s', color=colors[0], label='non-activated')]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(2,2))\n",
    "\n",
    "xLabel=[i+1 for i in range(LAYERS//2)]\n",
    "x = [(i+1)*2 for i in range(LAYERS//2)]\n",
    "\n",
    "\n",
    "ax.plot(x,np.array(non_counts[::2])/163.84,colors[0],marker='s',markersize='2.5') \n",
    "ax.tick_params(labelsize=8)    \n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(5))  \n",
    "\n",
    "\n",
    "fig.text(0.53, -0.05, 'Layers', ha='center',fontsize=8,fontweight='bold')\n",
    "fig.text(-0.1, 0.5, 'Percentage', va='center', rotation='vertical',fontsize=8,fontweight='bold')\n",
    "\n",
    "fig.legend(handles=legend_elements, bbox_to_anchor=(0.2, -0.07), loc=2, ncol=5,prop=legend_font)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7661c3b-40bf-44ad-8236-5b2456a1a330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9ecaee2-383e-4c94-9597-a82b2a7c08c8",
   "metadata": {},
   "source": [
    "## over threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d044496a-1d33-4a70-9d3a-a6b64f999b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import MultipleLocator\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "question_all,answer_all = load_data_XNLI()\n",
    "\n",
    "test_counts = len(question_all)\n",
    "langs_counts = len(question_all[0])\n",
    "\n",
    "thres_all_shared,thres_partial_shared,thres_non,thres_specific = [],[],[],[]\n",
    "for thres in [0,0.1,0.2,0.3,0.4,0.5]:\n",
    "    all_activated_counts,all_shared_counts,non_counts,specific_counts,all_partial_shared_counts = [],[],[],[] ,[]\n",
    "    for ind in range(test_counts):\n",
    "        sents = question_all[ind]\n",
    "        mlp_acts = []\n",
    "        for lang in range(len(sents)):\n",
    "            question = sents[lang]\n",
    "            encodings = tokenizer(question, return_tensors='pt')\n",
    "            input_ids = encodings['input_ids'].to('cuda')\n",
    "\n",
    "            if 'bloom' in model_name:\n",
    "                mlp_act = act_bloom(input_ids)\n",
    "            mlp_acts.append(mlp_act)\n",
    "\n",
    "        mlp_all_act = []\n",
    "        for i in range(langs_counts):\n",
    "            mlp_act = (mlp_acts[i]>thres).astype(int)\n",
    "            mlp_all_act.append(mlp_act)\n",
    "        mlp_all = np.sum(mlp_all_act,axis=0)    \n",
    "\n",
    "        all_activated,all_activated_count = [],[]\n",
    "        for i in range(langs_counts):   \n",
    "            activated,activated_count = [],[]\n",
    "            for ly in range(LAYERS):\n",
    "                act = torch.nonzero(torch.tensor(mlp_all_act[i][ly]==1)).squeeze(1).detach().cpu().numpy()\n",
    "                activated.append(act) \n",
    "                activated_count.append(len(act))\n",
    "            all_activated.append(activated)\n",
    "            all_activated_count.append(activated_count)\n",
    "\n",
    "\n",
    "        all_shared,all_shared_count = [],[]\n",
    "        for ly in range(LAYERS):\n",
    "            mlp_all_act_inter = torch.nonzero(torch.tensor(mlp_all[ly]==langs_counts)).squeeze(1).detach().cpu().numpy()\n",
    "            all_shared.append(mlp_all_act_inter)\n",
    "            all_shared_count.append(len(mlp_all_act_inter))\n",
    "\n",
    "        non,non_count = [],[]\n",
    "        mlp_all = np.sum(mlp_all_act,axis=0)\n",
    "        for ly in range(LAYERS):\n",
    "            mlp_all_act_inter = torch.nonzero(torch.tensor(mlp_all[ly]==0)).squeeze(1).detach().cpu().numpy()\n",
    "            non.append(mlp_all_act_inter)  \n",
    "            non_count.append(len(mlp_all_act_inter))\n",
    "\n",
    "\n",
    "        specific,specific_count = [],[]\n",
    "        for i in range(langs_counts):       \n",
    "            indices_arr1 = np.where(mlp_all_act[i] == 1)\n",
    "            indices_arr2 = np.where(mlp_all == 1)\n",
    "            set_indices_arr1 = set(zip(indices_arr1[0], indices_arr1[1]))\n",
    "            set_indices_arr2 = set(zip(indices_arr2[0], indices_arr2[1]))\n",
    "            intersection = set_indices_arr1.intersection(set_indices_arr2)\n",
    "            if len(intersection) > 0:\n",
    "                rows_intersection, cols_intersection = zip(*intersection)\n",
    "            else:\n",
    "                rows_intersection, cols_intersection = [],[]\n",
    "            row = [[] for _ in range(LAYERS)]\n",
    "            for k in range(len(rows_intersection)):\n",
    "                r,c = rows_intersection[k],cols_intersection[k]\n",
    "                row[r].append(c)\n",
    "            specific.append(row)\n",
    "\n",
    "\n",
    "        all_specific,specific_count = [],[]\n",
    "        for ly in range(LAYERS):\n",
    "            temp = []\n",
    "            for i in range(langs_counts):\n",
    "                temp += specific[i][ly]\n",
    "            all_specific.append(temp)\n",
    "            specific_count.append(len(temp))\n",
    "\n",
    "        partial_shared = []\n",
    "        for lg in range(langs_counts):\n",
    "            lg_some = []\n",
    "            for ly in range(LAYERS):\n",
    "                other = all_shared[ly].tolist() + specific[lg][ly]\n",
    "                some = list((set(all_activated[lg][ly])-set(other)))\n",
    "                lg_some.append(some)\n",
    "            partial_shared.append(lg_some)\n",
    "\n",
    "        all_partial_shared,all_partial_shared_count = [],[]\n",
    "        for ly in range(LAYERS):\n",
    "            temp = []\n",
    "            for i in range(langs_counts):\n",
    "                temp += partial_shared[i][ly]\n",
    "            all_partial_shared.append(list(set(temp)))\n",
    "            all_partial_shared_count.append(len(list(set(temp))))\n",
    "\n",
    "        all_activated_counts.append(all_activated_count)\n",
    "        all_shared_counts.append(all_shared_count)\n",
    "        non_counts.append(non_count)\n",
    "        specific_counts.append(specific_count)\n",
    "        all_partial_shared_counts.append(all_partial_shared_count)\n",
    "\n",
    "\n",
    "\n",
    "    all_activated_counts,all_shared_counts,non_counts,specific_counts,all_partial_shared_counts = mean(all_activated_counts, axis=0),mean(all_shared_counts, axis=0),mean(non_counts, axis=0),mean(specific_counts, axis=0),mean(all_partial_shared_counts, axis=0)\n",
    "\n",
    "    thres_all_shared.append(all_shared_counts)\n",
    "    thres_partial_shared.append(all_partial_shared_counts)\n",
    "    thres_non.append(non_counts)\n",
    "    thres_specific.append(specific_counts)\n",
    "print(np.array(thres_all_shared).shape)\n",
    "\n",
    "\n",
    "legend_font = {\n",
    "'style': 'normal',\n",
    "'size': 8,\n",
    "'weight': \"bold\",  \n",
    "}\n",
    "thres = [0,0.1,0.2,0.3,0.4,0.5]\n",
    "\n",
    "legend_elements = [\n",
    "            Line2D([0], [0], marker='d', color=colors[1], label='all-shared'),\n",
    "            Line2D([0], [0], marker='<', color=colors[2], label='partial-shared'),\n",
    "            Line2D([0], [0], marker='o', color=colors[3], label='specific'),\n",
    "            # Line2D([0], [0], marker='s', color=colors[0], label='non-activated'),\n",
    "                  ]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2,3,figsize=(5,4))\n",
    "\n",
    "xLabel=[i+1 for i in range(LAYERS//2)]\n",
    "x = [(i+1)*2 for i in range(LAYERS//2)]\n",
    "\n",
    "\n",
    "\n",
    "ax[0,0].plot(x,np.array(thres_all_shared[0][::2])/163.84,colors[1],marker='d',markersize='2.5')\n",
    "ax[0,0].plot(x,np.array(thres_partial_shared[0][::2])/163.84,colors[2],marker='<',markersize='2.5')\n",
    "ax[0,0].plot(x,np.array(thres_specific[0][::2])/163.84,colors[3],marker='o',markersize='2.5')\n",
    "# ax[0,0].plot(x,np.array(thres_non[0][::2])/163.84,colors[0],marker='s',markersize='2.5')\n",
    "ax[0,0].set_title(f'Threshold 0',fontsize=7,fontweight='bold')\n",
    "ax[0,0].tick_params(labelsize=8)  \n",
    "ax[0,0].xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "\n",
    "ax[0,1].plot(x,np.array(thres_all_shared[1][::2])/163.84,colors[1],marker='d',markersize='2.5')\n",
    "ax[0,1].plot(x,np.array(thres_partial_shared[1][::2])/163.84,colors[2],marker='<',markersize='2.5')\n",
    "ax[0,1].plot(x,np.array(thres_specific[1][::2])/163.84,colors[3],marker='o',markersize='2.5')\n",
    "# ax[0,1].plot(x,np.array(thres_non[1][::2])/163.84,colors[0],marker='s',markersize='2.5')\n",
    "ax[0,1].set_title(f'Threshold 0.1',fontsize=7,fontweight='bold')\n",
    "ax[0,1].tick_params(labelsize=8)  \n",
    "ax[0,1].xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "\n",
    "ax[0,2].plot(x,np.array(thres_all_shared[2][::2])/163.84,colors[1],marker='d',markersize='2.5')\n",
    "ax[0,2].plot(x,np.array(thres_partial_shared[2][::2])/163.84,colors[2],marker='<',markersize='2.5')\n",
    "ax[0,2].plot(x,np.array(thres_specific[2][::2])/163.84,colors[3],marker='o',markersize='2.5')\n",
    "# ax[0,2].plot(x,np.array(thres_non[2][::2])/163.84,colors[0],marker='s',markersize='2.5')\n",
    "ax[0,2].set_title(f'Threshold 0.2',fontsize=7,fontweight='bold')\n",
    "ax[0,2].tick_params(labelsize=8)  \n",
    "ax[0,2].xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "\n",
    "ax[1,0].plot(x,np.array(thres_all_shared[3][::2])/163.84,colors[1],marker='d',markersize='2.5')\n",
    "ax[1,0].plot(x,np.array(thres_partial_shared[3][::2])/163.84,colors[2],marker='<',markersize='2.5')\n",
    "ax[1,0].plot(x,np.array(thres_specific[3][::2])/163.84,colors[3],marker='o',markersize='2.5')\n",
    "# ax[1,0].plot(x,np.array(thres_non[3][::2])/163.84,colors[0],marker='s',markersize='2.5')\n",
    "ax[1,0].set_title(f'Threshold 0.3',fontsize=7,fontweight='bold')\n",
    "ax[1,0].tick_params(labelsize=8)  \n",
    "ax[1,0].xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "\n",
    "ax[1,1].plot(x,np.array(thres_all_shared[4][::2])/163.84,colors[1],marker='d',markersize='2.5')\n",
    "ax[1,1].plot(x,np.array(thres_partial_shared[4][::2])/163.84,colors[2],marker='<',markersize='2.5')\n",
    "ax[1,1].plot(x,np.array(thres_specific[4][::2])/163.84,colors[3],marker='o',markersize='2.5')\n",
    "# ax[1,1].plot(x,np.array(thres_non[4][::2])/163.84,colors[0],marker='s',markersize='2.5')\n",
    "ax[1,1].set_title(f'Threshold 0.4',fontsize=7,fontweight='bold')\n",
    "ax[1,1].tick_params(labelsize=8)  \n",
    "ax[1,1].xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "\n",
    "ax[1,2].plot(x,np.array(thres_all_shared[5][::2])/163.84,colors[1],marker='d',markersize='2.5')\n",
    "ax[1,2].plot(x,np.array(thres_partial_shared[5][::2])/163.84,colors[2],marker='<',markersize='2.5')\n",
    "ax[1,2].plot(x,np.array(thres_specific[5][::2])/163.84,colors[3],marker='o',markersize='2.5')\n",
    "# ax[1,2].plot(x,np.array(thres_non[5][::2])/163.84,colors[0],marker='s',markersize='2.5')\n",
    "ax[1,2].set_title(f'Threshold 0.5',fontsize=7,fontweight='bold')\n",
    "ax[1,2].tick_params(labelsize=8)  \n",
    "ax[1,2].xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "\n",
    "\n",
    "fig.text(0.53, 0.03, 'Layers', ha='center',fontsize=8,fontweight='bold')\n",
    "fig.text(0.03, 0.5, 'Percentage', va='center', rotation='vertical',fontsize=8,fontweight='bold')\n",
    "fig.subplots_adjust(wspace=0.25,hspace=0.35) \n",
    "\n",
    "fig.legend(handles=legend_elements, bbox_to_anchor=(0.15, -0.05), loc=3, ncol=3,prop=legend_font)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b78ecf-16e6-49a4-81f2-c772cf1ea4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remake (Conda)",
   "language": "python",
   "name": "sys_remake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
