{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1583211-a836-4d92-9f49-7e1c3061564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bloomz-7b1mt\"\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM,GenerationConfig,AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
    "MODEL = 'bloomz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de9396f-718d-46f2-a431-1ea6b6885426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baukit import Trace, TraceDict\n",
    "\n",
    "def get_out_bloomz(model, prompt, device,index): \n",
    "\n",
    "    model.eval()\n",
    "    MLP_act = [f\"transformer.h.{i}.mlp.gelu_impl\" for i in range(30)]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with TraceDict(model, MLP_act) as ret:\n",
    "            output = model(prompt, output_hidden_states = True,output_attentions=True)\n",
    "        MLP_act_value = [ret[act_value].output for act_value in MLP_act]\n",
    "        return MLP_act_value\n",
    "    \n",
    "    \n",
    "def act_bloom(input_ids):\n",
    "    mlp_act = get_out_bloomz(model,input_ids,model.device,-1)\n",
    "    mlp_act = np.array(mlp_act)\n",
    "    return mlp_act\n",
    "\n",
    "if 'bloom' in MODEL:\n",
    "    LAYERS = model.config.n_layer\n",
    "    Neuron_num = 16384\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e23ed18-8dc1-454d-bdc1-c28c448335f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = ['en','de','es','fr','ru','th','tr','vi','zh','pt']\n",
    "colors = ['cornflowerblue','forestgreen','orange','red','mediumturquoise','midnightblue','brown','moccasin','darkviolet','gold','deeppink','gray','teal','slateblue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e4ca12-269a-446e-a445-acaf111859dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38b9809e-599d-458e-a18b-c8b532297840",
   "metadata": {},
   "source": [
    "## XNLI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced24edb-b21f-4296-ab03-71c91ee527b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_XNLI():\n",
    "    with open(\"./mnli/xnli.test.tsv\") as f:\n",
    "        lines = f.readlines()\n",
    "    copora = [[],[],[],[],[]]\n",
    "    for line in lines[1:]:\n",
    "        line = line.split('\\t')\n",
    "        lang,label,sent1,sent2 = line[0],line[1],line[6],line[7]\n",
    "        if lang in langs:\n",
    "            ind = langs.index(lang)\n",
    "            res = (label,sent1,sent2)\n",
    "            copora[ind].append(res)\n",
    "        \n",
    "    length = len(copora[0])\n",
    "    question_all, answer_all = [],[]\n",
    "    for i in range(length):\n",
    "        question_all.append([])\n",
    "        answer_all.append([])\n",
    "    for ind in range(length):\n",
    "        for l in range(len(copora)):\n",
    "            tup = copora[l]\n",
    "            label,premise,hypothesis = tup[ind][0],tup[ind][1],tup[ind][2]\n",
    "            prompt = f'Take the following as truth: {premise}\\nThen the following statement: \"{hypothesis}\" is \"true\", \"false\", or \"inconclusive\"?'\n",
    "            question_all[ind].append(prompt)\n",
    "            answer_all[ind].append(label)\n",
    "    question_all, answer_all = question_all[:1000], answer_all[:1000]    \n",
    "    return question_all,answer_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044ff893-81b0-4521-a6e9-c99e4c26cec4",
   "metadata": {},
   "source": [
    "## Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0697fbbf-d96b-4ca4-a6f3-a9b9b3fe912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "import numpy\n",
    "\n",
    "class ModelingRequests():\n",
    "    def __init__(self):\n",
    "        self.model = model\n",
    "        self.device = 'cuda'\n",
    "        self.tokenizer = tokenizer        \n",
    "        self.TOP_K = 800\n",
    "        self.wte = model.transformer.word_embeddings.weight\n",
    "        \n",
    "    def cal_prompt(self,prompt):\n",
    "        self.to_hidden_states, self.to_preds = self.get_preds_and_hidden_states(prompt)\n",
    "        \n",
    "    def set_hooks_gpt2(self):\n",
    "        final_layer = self.model.config.n_layer - 1\n",
    "\n",
    "        for attr in [\"activations_\"]:\n",
    "            if not hasattr(self.model, attr):\n",
    "                setattr(self.model, attr, {})\n",
    "\n",
    "        def get_activation(name):\n",
    "            def hook(module, input, output):\n",
    "                if \"mlp\" in name or \"attn\" in name or \"m_coef\" in name:\n",
    "                    if \"attn\" in name:\n",
    "                        num_tokens = list(output[0].size())[1]\n",
    "                        self.model.activations_[name] = output[0][:, num_tokens - 1].detach()\n",
    "                    elif \"mlp\" in name:\n",
    "                        num_tokens = list(output[0].size())[0]  # [num_tokens, 3072] for values;\n",
    "                        self.model.activations_[name] = output[0][num_tokens - 1].detach()\n",
    "                    elif \"m_coef\" in name:\n",
    "                        num_tokens = list(input[0].size())[1]  # (batch, sequence, hidden_state)\n",
    "                        self.model.activations_[name] = input[0][:, num_tokens - 1].detach()\n",
    "                elif \"residual\" in name or \"embedding\" in name:\n",
    "                    num_tokens = list(input[0].size())[1]  # (batch, sequence, hidden_state)\n",
    "                    if name == \"layer_residual_\" + str(final_layer):\n",
    "                        self.model.activations_[name] = self.model.activations_[\n",
    "                                                            \"intermediate_residual_\" + str(final_layer)] + \\\n",
    "                                                        self.model.activations_[\"mlp_\" + str(final_layer)]\n",
    "\n",
    "                    else:\n",
    "                        self.model.activations_[name] = input[0][:,\n",
    "                                                        num_tokens - 1].detach()\n",
    "\n",
    "            return hook\n",
    "\n",
    "        self.model.transformer.h[0].input_layernorm.register_forward_hook(get_activation(\"input_embedding\"))\n",
    "\n",
    "        for i in range(self.model.config.n_layer):\n",
    "            if i != 0:\n",
    "                self.model.transformer.h[i].input_layernorm.register_forward_hook(get_activation(\"layer_residual_\" + str(i - 1)))\n",
    "            self.model.transformer.h[i].post_attention_layernorm.register_forward_hook(get_activation(\"intermediate_residual_\" + str(i)))\n",
    "\n",
    "            self.model.transformer.h[i].self_attention.register_forward_hook(get_activation(\"attn_\" + str(i)))\n",
    "            self.model.transformer.h[i].mlp.register_forward_hook(get_activation(\"mlp_\" + str(i)))\n",
    "            self.model.transformer.h[i].mlp.dense_4h_to_h.register_forward_hook(get_activation(\"m_coef_\" + str(i)))\n",
    "\n",
    "        self.model.transformer.ln_f.register_forward_hook(get_activation(\"layer_residual_\" + str(final_layer)))\n",
    "\n",
    "    def get_resid_predictions(self,sentence, start_idx=None, end_idx=None, set_mlp_0=False):\n",
    "        HIDDEN_SIZE = self.model.config.hidden_size\n",
    "\n",
    "        layer_residual_preds = []\n",
    "        intermed_residual_preds = []\n",
    "\n",
    "        if start_idx is not None and end_idx is not None:\n",
    "            tokens = [\n",
    "                token for token in sentence.split(' ')\n",
    "                if token not in ['', '\\n']\n",
    "            ]\n",
    "\n",
    "            sentence = \" \".join(tokens[start_idx:end_idx])\n",
    "        tokens = self.tokenizer(sentence, return_tensors=\"pt\")\n",
    "        tokens.to(self.device)\n",
    "        output = self.model(**tokens, output_hidden_states=True)\n",
    "        for layer in self.model.activations_.keys():\n",
    "            if \"layer_residual\" in layer or \"intermediate_residual\" in layer:\n",
    "                normed = self.model.transformer.ln_f(self.model.activations_[layer])\n",
    "\n",
    "                logits = torch.matmul(self.model.lm_head.weight, normed.T)\n",
    "\n",
    "                probs = F.softmax(logits.T[0], dim=-1)\n",
    "\n",
    "                probs = torch.reshape(probs, (-1,)).detach().cpu().numpy()\n",
    "\n",
    "                assert np.abs(np.sum(probs) - 1) <= 0.01, str(np.abs(np.sum(probs) - 1)) + layer\n",
    "\n",
    "                probs_ = []\n",
    "                for index, prob in enumerate(probs):\n",
    "                    probs_.append((index, prob))\n",
    "                top_k = sorted(probs_, key=lambda x: x[1], reverse=True)[:self.TOP_K]\n",
    "                top_k = [(t[1].item(), self.tokenizer.decode(t[0])) for t in top_k]\n",
    "            if \"layer_residual\" in layer:\n",
    "                layer_residual_preds.append(top_k)\n",
    "            elif \"intermediate_residual\" in layer:\n",
    "                intermed_residual_preds.append(top_k)\n",
    "\n",
    "            for attr in [\"layer_resid_preds\", \"intermed_residual_preds\"]:\n",
    "                if not hasattr(self.model, attr):\n",
    "                    setattr(self.model, attr, [])\n",
    "\n",
    "            self.model.layer_resid_preds = layer_residual_preds\n",
    "            self.model.intermed_residual_preds = intermed_residual_preds\n",
    "\n",
    "    def get_preds_and_hidden_states(self,prompt):\n",
    "        self.set_hooks_gpt2()\n",
    "\n",
    "        sent_to_preds = {}\n",
    "        sent_to_hidden_states = {}\n",
    "        sentence = prompt[:]\n",
    "        self.get_resid_predictions(sentence)\n",
    "        sent_to_preds[\"layer_resid_preds\"] = self.model.layer_resid_preds\n",
    "        sent_to_preds[\"intermed_residual_preds\"] = self.model.intermed_residual_preds\n",
    "        sent_to_hidden_states = self.model.activations_.copy()\n",
    "\n",
    "        return sent_to_hidden_states, sent_to_preds\n",
    "\n",
    "    def process_and_get_data(self,prompt):\n",
    "        sent_to_hidden_states, sent_to_preds = self.get_preds_and_hidden_states(prompt)\n",
    "        records = []\n",
    "        top_coef_idx = []\n",
    "        top_coef_vals = []\n",
    "        residual_preds_probs = []\n",
    "        residual_preds_tokens = []\n",
    "        layer_preds_probs = []\n",
    "        layer_preds_tokens = []\n",
    "        for LAYER in range(self.model.config.n_layer):\n",
    "            coefs_ = []\n",
    "            m_coefs = sent_to_hidden_states[\"m_coef_\" + str(LAYER)].squeeze(0).cpu().numpy()\n",
    "            res_vec = sent_to_hidden_states[\"layer_residual_\" + str(LAYER)].squeeze(0).cpu().numpy()\n",
    "            value_norms = torch.linalg.norm(self.model.transformer.h[LAYER].mlp.dense_4h_to_h.weight.t().data, dim=1).cpu()\n",
    "            # scaled_coefs = np.absolute(m_coefs) * value_norms.numpy()\n",
    "            scaled_coefs = m_coefs\n",
    "\n",
    "            for index, prob in enumerate(scaled_coefs):\n",
    "                coefs_.append((index, prob))\n",
    "\n",
    "            top_values = sorted(coefs_, key=lambda x: x[1], reverse=True)[:self.TOP_K]\n",
    "            c_idx, c_vals = zip(*top_values)\n",
    "            top_coef_idx.append(c_idx)\n",
    "            top_coef_vals.append(c_vals)\n",
    "            residual_p_probs, residual_p_tokens = zip(*sent_to_preds['intermed_residual_preds'][LAYER])\n",
    "            residual_preds_probs.append(residual_p_probs)\n",
    "            residual_preds_tokens.append(residual_p_tokens)\n",
    "\n",
    "            layer_p_probs, layer_p_tokens = zip(*sent_to_preds['layer_resid_preds'][LAYER])\n",
    "            layer_preds_probs.append(layer_p_probs)\n",
    "            layer_preds_tokens.append(layer_p_tokens)\n",
    "\n",
    "        return {\n",
    "            \"sent\": prompt,\n",
    "            \"top_coef_idx\": top_coef_idx,\n",
    "            \"top_coef_vals\": top_coef_vals,\n",
    "            \"residual_preds_probs\": residual_preds_probs,\n",
    "            \"residual_preds_tokens\": residual_preds_tokens,\n",
    "            \"layer_preds_probs\": layer_preds_probs,\n",
    "            \"layer_preds_tokens\": layer_preds_tokens,\n",
    "            \"layer_residual_vec\": res_vec,\n",
    "        }\n",
    "    \n",
    "    def contribution(self, indexs): # indexs应该是二维的，每层需要计算的neuron id\n",
    "        # sent_to_hidden_states, sent_to_preds = self.get_preds_and_hidden_states(prompt)\n",
    "        top_contribution_idx = []\n",
    "        top_contribution_vals = []\n",
    "        for LAYER in range(self.model.config.n_layer):\n",
    "            index_layer = indexs[LAYER]\n",
    "            coefs_ = []\n",
    "            m_coefs = self.to_hidden_states[\"m_coef_\" + str(LAYER)].squeeze(0).cpu().numpy()\n",
    "            value_norms = torch.linalg.norm(self.model.transformer.h[LAYER].mlp.dense_4h_to_h.weight.t().data, dim=1).cpu()\n",
    "            scaled_coefs = np.absolute(m_coefs) * value_norms.numpy()\n",
    "            # scaled_coefs = m_coefs * value_norms.numpy()\n",
    "            \n",
    "            for index, prob in enumerate(scaled_coefs):\n",
    "                if index in index_layer:\n",
    "                    coefs_.append((index, prob))\n",
    "            if len(coefs_) > 0:\n",
    "                top_values = sorted(coefs_, key=lambda x: x[1], reverse=True)[:self.TOP_K]\n",
    "                c_idx, c_vals = zip(*top_values)\n",
    "                top_contribution_idx.append(list(c_idx))\n",
    "                top_contribution_vals.append(list(c_vals))\n",
    "            else:\n",
    "                top_contribution_idx.append([])\n",
    "                top_contribution_vals.append([])\n",
    "        return {\n",
    "            \"top_contribution_idx\": top_contribution_idx,\n",
    "            \"top_contribution_vals\": top_contribution_vals,\n",
    "        }\n",
    "    def all_contribution_values(self, indexs): \n",
    "        top_contribution_idx = []\n",
    "        contribution_vals = []\n",
    "        for LAYER in range(self.model.config.n_layer):\n",
    "            index_layer = indexs[LAYER]\n",
    "            coefs_ = []\n",
    "            m_coefs = self.to_hidden_states[\"m_coef_\" + str(LAYER)].squeeze(0).cpu().numpy()\n",
    "            value_norms = torch.linalg.norm(self.model.transformer.h[LAYER].mlp.dense_4h_to_h.weight.t().data, dim=1).cpu()\n",
    "            scaled_coefs = np.absolute(m_coefs) * value_norms.numpy()\n",
    "            # scaled_coefs = m_coefs * value_norms.numpy()\n",
    "            \n",
    "            contribution_vals.append(scaled_coefs)\n",
    "\n",
    "        return {\n",
    "            \"contribution_vals\": contribution_vals,\n",
    "        }\n",
    "    def effective_score(self,token_id, neuron_idxs,layer_idxs):\n",
    "        scores = []\n",
    "        for LAYER in layer_idxs:\n",
    "            m_coefs = self.to_hidden_states[\"m_coef_\" + str(LAYER)].squeeze(0).cpu().numpy()\n",
    "            value = self.model.transformer.h[LAYER].mlp.dense_4h_to_h.weight.t().data.cpu().numpy()\n",
    "            effect = m_coefs.reshape(16384,1) * value\n",
    "            # print(np.array(effect).shape)\n",
    "            score = self.wte[token_id] @ torch.tensor(effect.T).to('cuda')\n",
    "            scores.append(score.detach().cpu().numpy())\n",
    "        return scores\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bde757e-3201-4075-8a0c-a93d0ec1006e",
   "metadata": {},
   "source": [
    "## Contribution Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f69a52-54c5-4f96-a9c9-20ab71ff2ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "\n",
    "question_all,answer_all = load_data_XNLI()\n",
    "\n",
    "questions,answers = length_filter(question_all,answer_all)\n",
    "test_counts = len(questions)\n",
    "langs_counts = len(questions[0])\n",
    "\n",
    "\n",
    "ratios = []\n",
    "request = ModelingRequests()\n",
    "request.TOP_K=800\n",
    "\n",
    "for q in range(test_counts):\n",
    "    print(q)\n",
    "    sents,anss = questions[q],answers[q]\n",
    "    mlp_acts,mlp_ups,mlp_ups_acts = [],[],[]\n",
    "    for sent in sents:\n",
    "        encodings = tokenizer(sent, return_tensors='pt')\n",
    "        input_ids = encodings['input_ids'].to('cuda')\n",
    "\n",
    "        if 'bloom' in MODEL:\n",
    "            mlp_act = act_bloom(input_ids)\n",
    "        mlp_acts.append(mlp_act)\n",
    "\n",
    "\n",
    "    mlp_all_act = []\n",
    "    for i in range(langs_counts):\n",
    "        mlp_act = (mlp_acts[i]>0).astype(int)\n",
    "        mlp_all_act.append(mlp_act)\n",
    "    mlp_all = np.sum(mlp_all_act,axis=0)\n",
    "\n",
    "    all_activated = []\n",
    "    for i in range(langs_counts):   \n",
    "        activated = []\n",
    "        for ly in range(LAYERS):\n",
    "            act = torch.nonzero(torch.tensor(mlp_all_act[i][ly]==1)).squeeze(1).detach().cpu().numpy()\n",
    "            activated.append(act) \n",
    "        all_activated.append(activated)\n",
    "\n",
    "    all_shared = []\n",
    "    for ly in range(LAYERS):\n",
    "        mlp_all_act_inter = torch.nonzero(torch.tensor(mlp_all[ly]==langs_counts)).squeeze(1).detach().cpu().numpy()\n",
    "        all_shared.append(mlp_all_act_inter)\n",
    "\n",
    "\n",
    "    non = []\n",
    "    mlp_all = np.sum(mlp_all_act,axis=0)\n",
    "    for ly in range(LAYERS):\n",
    "        mlp_all_act_inter = torch.nonzero(torch.tensor(mlp_all[ly]==0)).squeeze(1).detach().cpu().numpy()\n",
    "        non.append(mlp_all_act_inter)  \n",
    "\n",
    "\n",
    "    specific = []\n",
    "    for i in range(langs_counts):       \n",
    "        indices_arr1 = np.where(mlp_all_act[i] == 1)\n",
    "        indices_arr2 = np.where(mlp_all == 1)\n",
    "        set_indices_arr1 = set(zip(indices_arr1[0], indices_arr1[1]))\n",
    "        set_indices_arr2 = set(zip(indices_arr2[0], indices_arr2[1]))\n",
    "        intersection = set_indices_arr1.intersection(set_indices_arr2)\n",
    "        if len(intersection) > 0:\n",
    "            rows_intersection, cols_intersection = zip(*intersection)\n",
    "        else:\n",
    "            rows_intersection, cols_intersection = [], []\n",
    "        row = [[] for _ in range(LAYERS)]\n",
    "        for k in range(len(rows_intersection)):\n",
    "            r,c = rows_intersection[k],cols_intersection[k]\n",
    "            row[r].append(c)\n",
    "        specific.append(row)\n",
    "\n",
    "\n",
    "    all_specific = []\n",
    "    for ly in range(LAYERS):\n",
    "        specific_ly = []\n",
    "        for lg in range(langs_counts):\n",
    "            specific_ly += specific[lg][ly]\n",
    "        all_specific.append(specific_ly)\n",
    "\n",
    "    partial_shared = []\n",
    "    all = [i for i in range(16384)]\n",
    "\n",
    "    for ly in range(LAYERS):\n",
    "        other = all_shared[ly].tolist() + non[ly].tolist() + all_specific[ly]\n",
    "        some = list((set(all)-set(other)))\n",
    "        partial_shared.append(some)\n",
    "\n",
    "    lang_ratio = [] \n",
    "    for ind in range(len(sents)):\n",
    "        sent = sents[ind]        \n",
    "\n",
    "        request.cal_prompt(sent)\n",
    "        indexs = [[i for i in range(16384)] for _ in range(LAYERS)]\n",
    "        cont_id = request.contribution(indexs)['top_contribution_idx']\n",
    "        ratio = [] \n",
    "        for ly in range(LAYERS):\n",
    "            ratio_layer = [0,0,0,0]\n",
    "            cont_layer_ids = cont_id[ly]\n",
    "            all_shared_layer = all_shared[ly]\n",
    "            non_layer = non[ly]\n",
    "            specific_layer = all_specific[ly]\n",
    "            partial_shared_layer = partial_shared[ly]\n",
    "            for c_id in cont_layer_ids:\n",
    "                if c_id in all_shared_layer:\n",
    "                    ratio_layer[0] +=1\n",
    "                elif c_id in partial_shared_layer:\n",
    "                    ratio_layer[1] +=1\n",
    "                elif c_id in specific_layer:\n",
    "                    ratio_layer[2] +=1\n",
    "                elif c_id in non_layer:\n",
    "                    ratio_layer[3] +=1\n",
    "            ratio_layer = np.array(ratio_layer)/800\n",
    "            ratio.append(ratio_layer.tolist())\n",
    "        lang_ratio.append(ratio)\n",
    "    ratios.append(lang_ratio)\n",
    "ratios = np.mean(ratios,axis=0)\n",
    "ratios = np.array(ratios)*100\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import MultipleLocator\n",
    "\n",
    "legend_font = {\n",
    "    'style': 'normal',\n",
    "    'size': 9,  \n",
    "    'weight': \"bold\", \n",
    "}\n",
    "\n",
    "\n",
    "all_shared_data = ratios[:,:,0]\n",
    "partial_shared_data = ratios[:,:,1]\n",
    "specific_data = ratios[:,:,2]\n",
    "non_data = ratios[:,:,3]\n",
    "\n",
    "xLabel=[i+1 for i in range(LAYERS)]\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "x = [i+1 for i in range(LAYERS)]\n",
    "\n",
    "for i in range(1, langs_counts+1):\n",
    "    plt.subplot(2, 5, i)\n",
    "    plt.bar(x,all_shared_data[i-1],color=\"#59A14F\",label=\"All-shared Neuron\")\n",
    "    plt.bar(x,partial_shared_data[i-1],color=\"#F28E2B\",bottom=np.array(all_shared_data[i-1]),label=\"Partial-shared Neuron\")\n",
    "    plt.bar(x,specific_data[i-1],color=\"#E15759\",bottom=np.array(all_shared_data[i-1])+np.array(partial_shared_data[i-1]),label=\"Specific Neuron\")\n",
    "    plt.bar(x,non_data[i-1],color=\"#A0CBE8\",bottom=np.array(all_shared_data[i-1])+np.array(partial_shared_data[i-1])+np.array(specific_data[i-1]),label=\"Non-activated Neuron\")\n",
    "    plt.title(langs[i-1],fontsize=9,fontweight='bold')\n",
    "    plt.xticks(xLabel,fontsize=6.7,fontweight='bold')\n",
    "    plt.yticks(fontsize=6.7,fontweight='bold')\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(5))\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(20))\n",
    "\n",
    "fig.text(0.5, 0.045, 'Layers', ha='center',fontsize=9,fontweight='bold')\n",
    "fig.text(0.08, 0.5, 'Contribution Ratio', va='center', rotation='vertical',fontsize=9,fontweight='bold')\n",
    "fig.subplots_adjust(wspace=0.23,hspace=0.25) \n",
    "\n",
    "lines, labels = fig.axes[0].get_legend_handles_labels()\n",
    "\n",
    "fig.legend(lines, labels, bbox_to_anchor=(0.5,-0.03), loc=8,prop=legend_font,ncol=4)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cb30ea-a783-4d50-82fc-7698d737aaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "\n",
    "\n",
    "question_all,answer_all = load_data_XNLI()\n",
    "\n",
    "questions,answers = length_filter(question_all,answer_all)\n",
    "test_counts = len(questions)\n",
    "langs_counts = len(questions[0])\n",
    "\n",
    "\n",
    "ratios = []\n",
    "as_mams,ps_mams,ls_mams,non_mams = [],[],[],[]\n",
    "request = ModelingRequests()\n",
    "\n",
    "\n",
    "for q in range(test_counts):\n",
    "    print(q)\n",
    "    sents,anss = questions[q],answers[q]\n",
    "    mlp_acts,mlp_ups,mlp_ups_acts = [],[],[]\n",
    "    for sent in sents:\n",
    "        encodings = tokenizer(sent, return_tensors='pt')\n",
    "        input_ids = encodings['input_ids'].to('cuda')\n",
    "\n",
    "        attention_mask = encodings['attention_mask'].to('cuda')\n",
    "\n",
    "        if 'bloom' in MODEL:\n",
    "            mlp_act = act_bloom(input_ids)\n",
    "        mlp_acts.append(mlp_act)\n",
    "\n",
    "\n",
    "    mlp_all_act = []\n",
    "    for i in range(langs_counts):\n",
    "        mlp_act = (mlp_acts[i]>0).astype(int)\n",
    "        mlp_all_act.append(mlp_act)\n",
    "    mlp_all = np.sum(mlp_all_act,axis=0)    \n",
    "\n",
    "    all_activated = []\n",
    "    for i in range(langs_counts):   \n",
    "        activated,activated_count = [],[]\n",
    "        for ly in range(LAYERS):\n",
    "            act = torch.nonzero(torch.tensor(mlp_all_act[i][ly]==1)).squeeze(1).detach().cpu().numpy()\n",
    "            activated.append(act) \n",
    "            activated_count.append(len(act))\n",
    "        all_activated.append(activated)\n",
    "\n",
    "    all_shared = []\n",
    "    for ly in range(LAYERS):\n",
    "        mlp_all_act_inter = torch.nonzero(torch.tensor(mlp_all[ly]==langs_counts)).squeeze(1).detach().cpu().numpy()\n",
    "        all_shared.append(mlp_all_act_inter)\n",
    "\n",
    "    non = []\n",
    "    mlp_all = np.sum(mlp_all_act,axis=0)\n",
    "    for ly in range(LAYERS):\n",
    "        mlp_all_act_inter = torch.nonzero(torch.tensor(mlp_all[ly]==0)).squeeze(1).detach().cpu().numpy()\n",
    "        non.append(mlp_all_act_inter)  \n",
    "\n",
    "    specific = []\n",
    "    for i in range(langs_counts):       \n",
    "        indices_arr1 = np.where(mlp_all_act[i] == 1)\n",
    "        indices_arr2 = np.where(mlp_all == 1)\n",
    "        set_indices_arr1 = set(zip(indices_arr1[0], indices_arr1[1]))\n",
    "        set_indices_arr2 = set(zip(indices_arr2[0], indices_arr2[1]))\n",
    "        intersection = set_indices_arr1.intersection(set_indices_arr2)\n",
    "        if len(intersection) > 0:\n",
    "            rows_intersection, cols_intersection = zip(*intersection)\n",
    "        else:\n",
    "            rows_intersection, cols_intersection = [],[]\n",
    "        row = [[] for _ in range(LAYERS)]\n",
    "        for k in range(len(rows_intersection)):\n",
    "            r,c = rows_intersection[k],cols_intersection[k]\n",
    "            row[r].append(c)\n",
    "        specific.append(row)\n",
    "\n",
    "\n",
    "    all_specific = []\n",
    "    for ly in range(LAYERS):\n",
    "        temp = []\n",
    "        for i in range(langs_counts):\n",
    "            temp += specific[i][ly]\n",
    "        all_specific.append(temp)\n",
    "\n",
    "\n",
    "    partial_shared = []\n",
    "    for lg in range(langs_counts):\n",
    "        lg_some = []\n",
    "        for ly in range(LAYERS):\n",
    "            other = all_shared[ly].tolist() + specific[lg][ly]\n",
    "            some = list((set(all_activated[lg][ly])-set(other)))\n",
    "            lg_some.append(some)\n",
    "        partial_shared.append(lg_some)\n",
    "\n",
    "    all_partial_shared = []\n",
    "    for ly in range(LAYERS):\n",
    "        temp = []\n",
    "        for i in range(langs_counts):\n",
    "            temp += partial_shared[i][ly]\n",
    "        all_partial_shared.append(list(set(temp)))\n",
    "\n",
    "    as_lang_mam,ps_lang_mam,ls_lang_mam,non_lang_mam = [],[],[],[]\n",
    "    for ind in range(len(sents)):\n",
    "        sent = sents[ind]        \n",
    "\n",
    "        request.cal_prompt(sent)\n",
    "        indexs = [[i for i in range(16384)] for _ in range(LAYERS)]\n",
    "        cont_value = request.all_contribution_values(indexs)['contribution_vals']\n",
    "        as_values,ps_values,ls_values,non_values = [],[],[],[]\n",
    "        for ly in range(LAYERS):\n",
    "            cont_layer_values = np.array(cont_value[ly])\n",
    "            all_shared_indices = all_shared[ly]\n",
    "            non_indices = non[ly]\n",
    "            specific_indices = all_specific[ly]\n",
    "            partial_shared_indices = all_partial_shared[ly]\n",
    "            all_shared_value,partial_shared_value,specific_value,non_value = cont_layer_values[all_shared_indices],cont_layer_values[partial_shared_indices],cont_layer_values[specific_indices],cont_layer_values[non_indices]            \n",
    "\n",
    "            if len(all_shared_value) > 0:\n",
    "                as_values.append([np.max(all_shared_value),np.mean(all_shared_value),np.min(all_shared_value),np.sum(all_shared_value)])\n",
    "            else:\n",
    "                as_values.append([0,0,0,0])\n",
    "            if len(partial_shared_value) > 0:\n",
    "                ps_values.append([np.max(partial_shared_value),np.mean(partial_shared_value),np.min(partial_shared_value),np.sum(partial_shared_value)])\n",
    "            else:\n",
    "                ps_values.append([0,0,0,0])\n",
    "            if len(specific_value) > 0:\n",
    "                ls_values.append([np.max(specific_value),np.mean(specific_value),np.min(specific_value),np.sum(specific_value)])\n",
    "            else:\n",
    "                ls_values.append([0,0,0,0])\n",
    "            if len(non_value) > 0:\n",
    "                non_values.append([np.max(non_value),np.mean(non_value),np.min(non_value),np.sum(non_value)])\n",
    "            else:\n",
    "                non_values.append([0,0,0,0])\n",
    "        as_lang_mam.append(as_values)\n",
    "        ps_lang_mam.append(ps_values)\n",
    "        ls_lang_mam.append(ls_values)\n",
    "        non_lang_mam.append(non_values) #10*30*3\n",
    "    as_mams.append(as_lang_mam)\n",
    "    ps_mams.append(ps_lang_mam)\n",
    "    ls_mams.append(ls_lang_mam)\n",
    "    non_mams.append(non_lang_mam)\n",
    "\n",
    "\n",
    "as_mams,ps_mams,ls_mams,non_mams = np.mean(as_mams,axis=0),np.mean(ps_mams,axis=0),np.mean(ls_mams,axis=0),np.mean(non_mams,axis=0)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import MultipleLocator\n",
    "\n",
    "legend_font = {\n",
    "    'style': 'normal',\n",
    "    'size': 9,  # 字号\n",
    "    'weight': \"bold\",  # 是否加粗，不加粗\n",
    "}\n",
    "\n",
    "\n",
    "avg_as_mams,avg_ps_mams,avg_ls_mams,avg_non_mams = as_mams[:,:,1],ps_mams[:,:,1],ls_mams[:,:,1],non_mams[:,:,1]\n",
    "sum_as_mams,sum_ps_mams,sum_ls_mams,sum_non_mams = as_mams[:,:,3],ps_mams[:,:,3],ls_mams[:,:,3],non_mams[:,:,3]\n",
    "\n",
    "xLabel=[i+1 for i in range(LAYERS)]\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "x = [i+1 for i in range(LAYERS)]\n",
    "\n",
    "for i in range(1, langs_counts+1):\n",
    "    plt.subplot(2, 5, i)\n",
    "    plt.plot(x,np.array(avg_as_mams[i-1]),color=\"#59A14F\",marker='s',markersize='2',label=\"All-shared Neuron\")   \n",
    "    plt.plot(x,np.array(avg_ps_mams[i-1]),color=\"#F28E2B\",label=\"Partial-shared Neuron\")\n",
    "    plt.plot(x,np.array(avg_ls_mams[i-1]),color=\"#E15759\",label=\"Specific Neuron\")\n",
    "    plt.plot(x,np.array(avg_non_mams[i-1]),color=\"#A0CBE8\",label=\"Non-activated Neuron\")\n",
    "    plt.title(langs[i-1],fontsize=9,fontweight='bold')\n",
    "    plt.xticks(xLabel,fontsize=6.7,fontweight='bold')\n",
    "    plt.yticks(fontsize=6.7,fontweight='bold')\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(5))\n",
    "\n",
    "fig.text(0.5, 0.045, 'Layers', ha='center',fontsize=9,fontweight='bold')\n",
    "fig.text(0.07, 0.5, 'Average Contribution Score', va='center', rotation='vertical',fontsize=9,fontweight='bold')\n",
    "fig.subplots_adjust(wspace=0.3,hspace=0.25) \n",
    "\n",
    "lines, labels = fig.axes[0].get_legend_handles_labels()\n",
    "\n",
    "fig.legend(lines, labels, bbox_to_anchor=(0.5,-0.03), loc=8,prop=legend_font,ncol=4)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "for i in range(1, langs_counts+1):\n",
    "    plt.subplot(2, 5, i)\n",
    "    plt.plot(x,np.array(sum_as_mams[i-1]),color=\"#59A14F\",marker='s',markersize='2',label=\"All-shared Neuron\")   \n",
    "    plt.plot(x,np.array(sum_ps_mams[i-1]),color=\"#F28E2B\",label=\"Partial-shared Neuron\")\n",
    "    plt.plot(x,np.array(sum_ls_mams[i-1]),color=\"#E15759\",label=\"Specific Neuron\")\n",
    "    plt.plot(x,np.array(sum_non_mams[i-1]),color=\"#A0CBE8\",label=\"Non-activated Neuron\")\n",
    "    plt.title(langs[i-1],fontsize=9,fontweight='bold')\n",
    "    plt.xticks(xLabel,fontsize=6.7,fontweight='bold')\n",
    "    plt.yticks(fontsize=6.7,fontweight='bold')\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(5))\n",
    "\n",
    "fig.text(0.5, 0.045, 'Layers', ha='center',fontsize=9,fontweight='bold')\n",
    "fig.text(0.07, 0.5, 'Sum of Contribution Score', va='center', rotation='vertical',fontsize=9,fontweight='bold')\n",
    "fig.subplots_adjust(wspace=0.3,hspace=0.25) \n",
    "\n",
    "lines, labels = fig.axes[0].get_legend_handles_labels()\n",
    "\n",
    "fig.legend(lines, labels, bbox_to_anchor=(0.5,-0.03), loc=8,prop=legend_font,ncol=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173d8b88-a533-4bf0-a043-2c31c0413dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "632a7fdb-29f3-46c9-987c-326944c7bda7",
   "metadata": {},
   "source": [
    "## Effective Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e2ad4d-8f84-466a-a068-4393f71ad56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "\n",
    "\n",
    "    question_all,answer_all = load_data_XNLI()\n",
    "        \n",
    "    questions,answers = length_filter(question_all,answer_all)\n",
    "    test_counts = len(questions)\n",
    "    langs_counts = len(questions[0])\n",
    "\n",
    "\n",
    "    ratios = []\n",
    "    as_mams,ps_mams,ls_mams,non_mams = [],[],[],[]\n",
    "    request = ModelingRequests_bloom()\n",
    "\n",
    "    for q in range(test_counts):\n",
    "        print(q)\n",
    "        sents,anss = questions[q],answers[q]\n",
    "        mlp_acts,mlp_ups,mlp_ups_acts = [],[],[]\n",
    "        for sent in sents:\n",
    "            encodings = tokenizer(sent, return_tensors='pt')\n",
    "            input_ids = encodings['input_ids'].to('cuda')\n",
    "\n",
    "            if 'bloom' in MODEL:\n",
    "                mlp_act = act_bloom(input_ids)\n",
    "            mlp_acts.append(mlp_act)\n",
    "\n",
    "\n",
    "        mlp_all_act = []\n",
    "        for i in range(langs_counts):\n",
    "            mlp_act = (mlp_acts[i]>0).astype(int)\n",
    "            mlp_all_act.append(mlp_act)\n",
    "        mlp_all = np.sum(mlp_all_act,axis=0)    \n",
    "\n",
    "        all_activated = []\n",
    "        for i in range(langs_counts):   \n",
    "            activated,activated_count = [],[]\n",
    "            for ly in range(LAYERS):\n",
    "                act = torch.nonzero(torch.tensor(mlp_all_act[i][ly]==1)).squeeze(1).detach().cpu().numpy()\n",
    "                activated.append(act) \n",
    "                activated_count.append(len(act))\n",
    "            all_activated.append(activated)\n",
    "\n",
    "        all_shared = []\n",
    "        for ly in range(LAYERS):\n",
    "            mlp_all_act_inter = torch.nonzero(torch.tensor(mlp_all[ly]==langs_counts)).squeeze(1).detach().cpu().numpy()\n",
    "            all_shared.append(mlp_all_act_inter)\n",
    "\n",
    "        non = []\n",
    "        mlp_all = np.sum(mlp_all_act,axis=0)\n",
    "        for ly in range(LAYERS):\n",
    "            mlp_all_act_inter = torch.nonzero(torch.tensor(mlp_all[ly]==0)).squeeze(1).detach().cpu().numpy()\n",
    "            non.append(mlp_all_act_inter)  \n",
    "\n",
    "        specific = []\n",
    "        for i in range(langs_counts):       \n",
    "            indices_arr1 = np.where(mlp_all_act[i] == 1)\n",
    "            indices_arr2 = np.where(mlp_all == 1)\n",
    "            set_indices_arr1 = set(zip(indices_arr1[0], indices_arr1[1]))\n",
    "            set_indices_arr2 = set(zip(indices_arr2[0], indices_arr2[1]))\n",
    "            intersection = set_indices_arr1.intersection(set_indices_arr2)\n",
    "            if len(intersection) > 0:\n",
    "                rows_intersection, cols_intersection = zip(*intersection)\n",
    "            else:\n",
    "                rows_intersection, cols_intersection = [],[]\n",
    "            row = [[] for _ in range(LAYERS)]\n",
    "            for k in range(len(rows_intersection)):\n",
    "                r,c = rows_intersection[k],cols_intersection[k]\n",
    "                row[r].append(c)\n",
    "            specific.append(row)\n",
    "            \n",
    "            \n",
    "        all_specific = []\n",
    "        for ly in range(LAYERS):\n",
    "            temp = []\n",
    "            for i in range(langs_counts):\n",
    "                temp += specific[i][ly]\n",
    "            all_specific.append(temp)\n",
    "        \n",
    "            \n",
    "        partial_shared = []\n",
    "        for lg in range(langs_counts):\n",
    "            lg_some = []\n",
    "            for ly in range(LAYERS):\n",
    "                other = all_shared[ly].tolist() + specific[lg][ly]\n",
    "                some = list((set(all_activated[lg][ly])-set(other)))\n",
    "                lg_some.append(some)\n",
    "            partial_shared.append(lg_some)\n",
    "\n",
    "        all_partial_shared = []\n",
    "        for ly in range(LAYERS):\n",
    "            temp = []\n",
    "            for i in range(langs_counts):\n",
    "                temp += partial_shared[i][ly]\n",
    "            all_partial_shared.append(list(set(temp)))\n",
    "            \n",
    "        as_lang_mam,ps_lang_mam,ls_lang_mam,non_lang_mam = [],[],[],[]    \n",
    "        for ind in range(len(sents)):\n",
    "            sent = sents[ind]\n",
    "            ans = anss[ind]\n",
    "            q_encodings = tokenizer(sent, return_tensors='pt')['input_ids']\n",
    "            if ind != 8:\n",
    "                a_encodings = tokenizer(sent + ' ' + ans, return_tensors='pt')['input_ids']\n",
    "            else:\n",
    "                a_encodings = tokenizer(sent + ans, return_tensors='pt')['input_ids']\n",
    "            reference_id = a_encodings[0][len(q_encodings[0])]\n",
    "\n",
    "            request.cal_prompt(sent)\n",
    "            effective_score_value = request.effective_score(reference_id,[i for i in range(30)])\n",
    "            as_values,ps_values,ls_values,non_values = [],[],[],[]\n",
    "\n",
    "            \n",
    "            \n",
    "            for ly in range(LAYERS):\n",
    "                effective_score_layer_values = np.array(effective_score_value[ly])\n",
    "                all_shared_indices = all_shared[ly]\n",
    "                non_indices = non[ly]\n",
    "                specific_indices = all_specific[ly]\n",
    "                partial_shared_indices = all_partial_shared[ly]\n",
    "                all_shared_value,partial_shared_value,specific_value,non_value = effective_score_layer_values[all_shared_indices],effective_score_layer_values[partial_shared_indices],effective_score_layer_values[specific_indices],effective_score_layer_values[non_indices]    \n",
    "                \n",
    "                if len(all_shared_value) > 0:\n",
    "                    as_values.append([np.max(all_shared_value),np.mean(all_shared_value),np.min(all_shared_value)])\n",
    "                else:\n",
    "                    as_values.append([0,0,0])\n",
    "                if len(partial_shared_value) > 0:\n",
    "                    ps_values.append([np.max(partial_shared_value),np.mean(partial_shared_value),np.min(partial_shared_value)])\n",
    "                else:\n",
    "                    ps_values.append([0,0,0])\n",
    "                if len(specific_value) > 0:\n",
    "                    ls_values.append([np.max(specific_value),np.mean(specific_value),np.min(specific_value)])\n",
    "                else:\n",
    "                    ls_values.append([0,0,0])\n",
    "                if len(non_value) > 0:\n",
    "                    non_values.append([np.max(non_value),np.mean(non_value),np.min(non_value)])\n",
    "                else:\n",
    "                    non_values.append([0,0,0])\n",
    "            as_lang_mam.append(as_values)\n",
    "            ps_lang_mam.append(ps_values)\n",
    "            ls_lang_mam.append(ls_values)\n",
    "            non_lang_mam.append(non_values) #10*30*3\n",
    "        as_mams.append(as_lang_mam)\n",
    "        ps_mams.append(ps_lang_mam)\n",
    "        ls_mams.append(ls_lang_mam)\n",
    "        non_mams.append(non_lang_mam)\n",
    "        \n",
    "    as_mams,ps_mams,ls_mams,non_mams = np.mean(as_mams,axis=0),np.mean(ps_mams,axis=0),np.mean(ls_mams,axis=0),np.mean(non_mams,axis=0)\n",
    "    \n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.pyplot import MultipleLocator\n",
    "    import matplotlib.ticker as ticker\n",
    "\n",
    "    legend_font = {\n",
    "        'style': 'normal',\n",
    "        'size': 9,  # 字号\n",
    "        'weight': \"bold\",  # 是否加粗，不加粗\n",
    "    }\n",
    "\n",
    "\n",
    "    max_as_mams,max_ps_mams,max_ls_mams,max_non_mams = as_mams[:,:,0],ps_mams[:,:,0],ls_mams[:,:,0],non_mams[:,:,0]\n",
    "\n",
    "    maxs = [max_as_mams,max_ps_mams,max_ls_mams,max_non_mams]\n",
    "    \n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "    x = [i+1 for i in range(30)]\n",
    "\n",
    "    axes = fig.subplots(nrows=2, ncols=2)\n",
    "\n",
    "    for tp in range(4):\n",
    "        if tp == 0:\n",
    "            ax = axes[0, 0]\n",
    "        if tp == 1:\n",
    "            ax = axes[0, 1]\n",
    "        if tp == 2:\n",
    "            ax = axes[1, 0]\n",
    "        if tp == 3:\n",
    "            ax = axes[1, 1]\n",
    "        for i in range(langs_counts):\n",
    "            ax.scatter(x, maxs[tp][i], s=20, c=colors[i+2], marker='+',label=langs[i])\n",
    "            ax.tick_params(axis='both',labelsize=6)\n",
    "\n",
    "\n",
    "    axes[0, 0].set_title('All-shared Neuron',fontsize=7,fontweight='bold')\n",
    "    axes[0, 1].set_title('Patrial-shared Neuron',fontsize=7,fontweight='bold')\n",
    "    axes[1, 0].set_title('Specific Neuron',fontsize=7,fontweight='bold')\n",
    "    axes[1, 1].set_title('Non-activated Neuron',fontsize=7,fontweight='bold')\n",
    "\n",
    "    fig.text(0.5, 0.02, 'Layers', ha='center',fontsize=8,fontweight='bold')\n",
    "    fig.text(0.01, 0.5, 'Effective Score of Neuron', va='center', rotation='vertical',fontsize=8,fontweight='bold')\n",
    "    fig.subplots_adjust(wspace=0.3,hspace=0.3) \n",
    "\n",
    "    axes[0, 0].xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "    axes[0, 1].xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "    axes[1, 0].xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "    axes[1, 1].xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "    lines, labels = fig.axes[-1].get_legend_handles_labels()\n",
    "\n",
    "    fig.legend(lines, labels, bbox_to_anchor=(0, 0), borderaxespad=0, loc=2, ncol=5,prop=legend_font)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"____________________________________________\")\n",
    "    print(as_mams,ps_mams,ls_mams,non_mams)\n",
    "\n",
    "                \n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remake (Conda)",
   "language": "python",
   "name": "sys_remake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
